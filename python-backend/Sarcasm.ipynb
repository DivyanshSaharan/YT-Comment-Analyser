{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Importing Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, LSTM, Dense, Dropout, Flatten, LayerNormalization, MultiHeadAttention, Add, Input\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "MMTQQflxIXAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing Dataset\n",
        "df = pd.read_csv(\"/content/unique_sarcasm_dataset.csv\")"
      ],
      "metadata": {
        "id": "sT0dQ1aSTAVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    # Remove non-alphabetic characters and convert to lowercase\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    text = text.lower().strip()\n",
        "    return text"
      ],
      "metadata": {
        "id": "0cplIsYdTFir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['text'] = df['text'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "I2LgCJWXTK_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_texts, test_texts, train_labels, test_labels = train_test_split(df['text'], df['label'], test_size=0.1, random_state=42)"
      ],
      "metadata": {
        "id": "vpOPhwnDTLu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization parameters\n",
        "vocab_size = 10000\n",
        "max_length = 32\n",
        "embedding_dim = 128"
      ],
      "metadata": {
        "id": "tC94Drx5TUj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
        "# tokenizer.fit_on_texts(train_texts)"
      ],
      "metadata": {
        "id": "7YE65gBTTVwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Load the tokenizer\n",
        "with open('tokenizer.pkl', 'rb') as file:\n",
        "    tokenizer = pickle.load(file)"
      ],
      "metadata": {
        "id": "p9CaRaw4Yp9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization and Padding\n",
        "train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
        "test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
        "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding='post', truncating='post')\n",
        "test_padded = pad_sequences(test_sequences, maxlen=max_length, padding='post', truncating='post')"
      ],
      "metadata": {
        "id": "cT_7VN2BTYvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class TransformerBlock(tf.keras.layers.Layer):\n",
        "#     def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "#         super(TransformerBlock, self).__init__()\n",
        "#         self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "#         self.ffn = Sequential([\n",
        "#             Dense(ff_dim, activation=\"relu\"),\n",
        "#             Dense(embed_dim)\n",
        "#         ])\n",
        "#         self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
        "#         self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
        "#         self.dropout1 = Dropout(rate)\n",
        "#         self.dropout2 = Dropout(rate)\n",
        "\n",
        "#     def call(self, inputs, training=False):\n",
        "#         attn_output = self.att(inputs, inputs)\n",
        "#         attn_output = self.dropout1(attn_output, training=training)\n",
        "#         out1 = self.layernorm1(inputs + attn_output)\n",
        "#         ffn_output = self.ffn(out1)\n",
        "#         ffn_output = self.dropout2(ffn_output, training=training)\n",
        "#         return self.layernorm2(out1 + ffn_output)"
      ],
      "metadata": {
        "id": "s7-tuoS3TcDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Embedding, Bidirectional, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Model parameters\n",
        "embedding_dim = 128\n",
        "lstm_units = 128  # LSTM units\n",
        "\n",
        "# Embedded Layer\n",
        "inputs = Input(shape=(max_length,))\n",
        "x = Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length)(inputs)\n",
        "\n",
        "# CNN Layer\n",
        "x = Conv1D(filters=128, kernel_size=5, activation='relu', padding='same')(x)\n",
        "x = MaxPooling1D(pool_size=2)(x)\n",
        "\n",
        "# LSTM Layer (output size is 128)\n",
        "x = Bidirectional(LSTM(lstm_units, return_sequences=False))(x)\n",
        "\n",
        "# Dense layer\n",
        "x = Dense(64, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "\n",
        "# Output layer for binary classification (sarcastic or not)\n",
        "outputs = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "# Build and compile the model\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47XtoF6ITfLg",
        "outputId": "11117dc2-3f57-4678-ab68-a5c24b07586e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_padded, train_labels, epochs=3, batch_size=1, validation_data=(test_padded, test_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kySNpj-MTs4c",
        "outputId": "6fe033d8-7e84-4a46-bfc6-8ff1b96070e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 38ms/step - accuracy: 0.5150 - loss: 0.6575 - val_accuracy: 1.0000 - val_loss: 0.0098\n",
            "Epoch 2/3\n",
            "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 37ms/step - accuracy: 0.9831 - loss: 0.0450 - val_accuracy: 1.0000 - val_loss: 0.0191\n",
            "Epoch 3/3\n",
            "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.0037 - val_accuracy: 0.9500 - val_loss: 0.1834\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_sarcasm(sentences):\n",
        "    # Preprocess and tokenize the input sentences\n",
        "    sentences = [preprocess_text(sentence) for sentence in sentences]\n",
        "    sequences = tokenizer.texts_to_sequences(sentences)\n",
        "    padded = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')\n",
        "\n",
        "    # Predicting sarcasm\n",
        "    predictions = model.predict(padded)\n",
        "    return predictions\n",
        "    return [\"Sarcastic\" if pred > 0.5 else \"Not Sarcastic\" for pred in predictions]\n",
        "\n",
        "# Example usage\n",
        "new_sentences = [\"this video is fantastic and enjoyable\", \"Oh fanastic another Monday doing this. Just what I wanted to do\"]\n",
        "predictions = predict_sarcasm(new_sentences)\n",
        "print(predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "RcJvnhuwTrfW",
        "outputId": "af527755-c704-4f77-da3e-27f6a6b6ea48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'preprocess_text' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c324e7aef7a1>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Example usage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mnew_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"this video is fantastic and enjoyable\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Oh fanastic another Monday doing this. Just what I wanted to do\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_sarcasm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_sentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-c324e7aef7a1>\u001b[0m in \u001b[0;36mpredict_sarcasm\u001b[0;34m(sentences)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict_sarcasm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Preprocess and tokenize the input sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpreprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0msequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mpadded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncating\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-c324e7aef7a1>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict_sarcasm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Preprocess and tokenize the input sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpreprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0msequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mpadded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncating\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'preprocess_text' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Storing Models Params"
      ],
      "metadata": {
        "id": "nxKBi8VkQpKH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the model as an .h5 file\n",
        "model.save('new_model_sarcasm.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1Bp5VvEQv11",
        "outputId": "d0d2c4d7-436c-422a-b60f-40a1821e8896"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    }
  ]
}